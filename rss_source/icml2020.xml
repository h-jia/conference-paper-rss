<?xml version="1.0" encoding="utf8"?>
<rss version="2.0">
<channel>
    <title>icml 2020</title>
    
    <item>
        <title>Selective Dyna-Style Planning Under Limited Model Capacity</title>
        <link>http://proceedings.mlr.press/v119/abbas20a/abbas20a.pdf</link>
        <description>
    In model-based reinforcement learning, planning with an imperfect model of the environment has the potential to harm learning progress. But even when a model is imperfect, it may still contain information that is useful for planning. In this paper, we investigate the idea of using an imperfect model selectively. The agent should plan in parts of the state space where the model would be helpful but refrain from using the model where it would be harmful. An effective selective planning mechanism requires estimating predictive uncertainty, which arises out of aleatoric uncertainty, parameter uncertainty, and model inadequacy, among other sources. Prior work has focused on parameter uncertainty for selective planning. In this work, we emphasize the importance of model inadequacy. We show that heteroscedastic regression can signal predictive uncertainty arising from model inadequacy that is complementary to that which is detected by methods designed for parameter uncertainty, indicating that considering both parameter uncertainty and model inadequacy may be a more promising direction for effective selective planning than either in isolation.
  </description>
    </item>
    
    <item>
        <title>A distributional view on multi-objective policy optimization</title>
        <link>http://proceedings.mlr.press/v119/abdolmaleki20a/abdolmaleki20a.pdf</link>
        <description>
    Many real-world problems require trading off multiple competing objectives. However, these objectives are often in different units and/or scales, which can make it challenging for practitioners to express numerical preferences over objectives in their native units. In this paper we propose a novel algorithm for multi-objective reinforcement learning that enables setting desired preferences for objectives in a scale-invariant way. We propose to learn an action distribution for each objective, and we use supervised learning to fit a parametric policy to a combination of these distributions. We demonstrate the effectiveness of our approach on challenging high-dimensional real and simulated robotics tasks, and show that setting different preferences in our framework allows us to trace out the space of nondominated solutions.
  </description>
    </item>
    
    
</channel>
</rss>
